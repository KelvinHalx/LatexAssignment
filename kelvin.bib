@inproceedings{10.1145/3194085.3194087,
author = {Rao, Qing and Frtunikj, Jelena},
title = {Deep Learning for Self-Driving Cars: Chances and Challenges},
year = {2018},
isbn = {9781450357395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194085.3194087},
doi = {10.1145/3194085.3194087},
abstract = {Artificial Intelligence (AI) is revolutionizing the modern society. In the automotive
industry, researchers and developers are actively pushing deep learning based approaches
for autonomous driving. However, before a neural network finds its way into series
production cars.Has to first undergo strict assessment concerning functional safety.
The chances and challenges of incorporating deep learning for self-driving cars are
presented in this paper.},
booktitle = {Proceedings of the 1st International Workshop on Software Engineering for AI in Autonomous Systems},
pages = {35–38},
numpages = {4},
keywords = {functional safety, automotive, deep learning},
location = {Gothenburg, Sweden},
series = {SEFAIS '18}
}

@article{HONG2021106944,
title = {AI, you can drive my car: How we evaluate human drivers vs. self-driving cars},
journal = {Computers in Human Behavior},
volume = {125},
pages = {106944},
year = {2021},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2021.106944},
url = {https://www.sciencedirect.com/science/article/pii/S0747563221002673},
author = {Joo-Wha Hong and Ignacio Cruz and Dmitri Williams},
keywords = {Self-driving cars, Schema theory, Computers-are-social-actors, Attribution theory, Human-agent communication, Human-computer interaction},
abstract = {This study tests how individuals attribute responsibility to an artificial intelligent (AI) agent or a human agent based on their involvement in a negative or positive event. In an online, vignette experimental between-subjects design, participants (n = 230) responded to a questionnaire measuring their opinions about the level of responsibility and involvement attributed to an AI agent or human agent across rescue (i.e., positive) or accident (i.e., negative) driving scenarios. Results show that individuals are more likely to attribute responsibility to an AI agent during rescues, or positive events. Also, we find that individuals perceive the actions of AI agents similarly to human agents, which supports CASA framework's claims that technologies can have agentic qualities. In order to explain why individuals do not always attribute full responsibility for an outcome to an AI agent, we use Expectancy Violation Theory to understand why people credit or blame artificial intelligence during unexpected events. Implications of findings for practical applications and theory are discussed.}
}
